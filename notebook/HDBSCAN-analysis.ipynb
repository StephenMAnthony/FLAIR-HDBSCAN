{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b9e1ee-a3c3-46e0-ab72-9c4bbadc3b6f",
   "metadata": {
    "id": "f631ae2d-c169-461c-937a-3446a78abe79",
    "tags": []
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1ygAs8EMNlIim2ypwmvQn9yN1LbY3hWHV\" alt=\"Drawing\"  width=\"30%\"/><center>\n",
    "\n",
    "# <center><strong>Development notebook</strong></center>\n",
    "<br/>\n",
    "\n",
    "<br/><center>This notebook allows you to visualize the data used in the **FLAIR #2 challenge**.<br/>The code bellow works with the toy dataset (subset) provided in the starting-kit alongside this notebook as well as with the full FLAIR-two dataset accessible after registration to the competition.</center> <br/> \n",
    "<center>**We also strongly advise you to read the data technical description provided in the datapaper.**</center>\n",
    "<br/> <br/> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a551697-f7df-4079-9719-d658971e8a34",
   "metadata": {},
   "source": [
    "Handle all the generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012f4c2-c578-4206-93ff-fe2f8ed0dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38dda1-76dd-4409-a682-a4866f1419ed",
   "metadata": {},
   "source": [
    "Import code from or based upon the FLAIR-2 reference implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc9829-c35d-42b0-b6f2-8c92b51748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAIR_path = join(Path.cwd().parents[0],'FLAIR-code/src')\n",
    "if FLAIR_path not in sys.path:\n",
    "    sys.path.append(FLAIR_path)\n",
    "\n",
    "from data_display import (display_nomenclature,\n",
    "                            display_samples, \n",
    "                            display_time_serie,\n",
    "                            display_all_with_semantic_class, \n",
    "                            display_all, \n",
    "                            read_dates, \n",
    "                            filter_dates)\n",
    "from load_data import load_data\n",
    "from FusedDataset import FusedDataset\n",
    "from calc_miou import calc_miou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ebbdb-2eda-4d14-85ff-8d5a30542764",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Load Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Use reference code to create lists containing the paths to the input images (`images`) and supervision masks (`masks`) files of the dataset.<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214019e8-6f88-438b-ab7f-5a62c3cc8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/app/FLAIR-HDBSCAN/flair-2-config.yml\" \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Creation of the train, val, and test dictionaries with the data file paths\n",
    "# Note that due to using the toy dataset we assign 100% of the data for training. \n",
    "# While not best practice for machine learning, for the toy dataset when using the stock code and less than 100%, issues randomly arise when using the reference loader\n",
    "# as the random selection can result in some semantic classes not being represented. If the full FLAIR #2 dataset were employed, validation data should be separate. \n",
    "# Due to size limitations on HDBSCAN, for actual training we downsample to ~1 % of the training data for training and use 100% of the training data for fitting. \n",
    "d_train, d_val, d_test = load_data(config, val_percent=1)\n",
    "\n",
    "# Convert to torch Datasets\n",
    "train_dataset = FusedDataset(dict_files=d_train, config=config)\n",
    "valid_dataset = FusedDataset(dict_files=d_val, config=config)\n",
    "test_dataset = FusedDataset(dict_files=d_val, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ca57a-be05-40c0-9360-86e06d64fa98",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-2: Development area</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Area under active development. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea08e0-9e4a-441f-af25-4d944924336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = join(Path.cwd().parents[0],'code')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6d3b1-bcec-44ec-b033-e1de8612a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import display\n",
    "reload(display)\n",
    "from display import display_confusion\n",
    "\n",
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import train_and_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbb02c-ad85-42ec-b332-6a7ada249d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933a1a4-1076-4a31-971e-f961a5e9a368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24b98a-15c6-4b46-92da-d89ed51943da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7017109-8388-4f1a-94b7-1a634543e802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f072f-2098-43cf-94a9-d28767e9272b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe15a6a-13bd-4b46-8c8b-04d6fb258009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b7271-372f-46fb-a902-226469e8f945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412d0d8-0979-47fd-a1e4-00c83d9639fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93535c67-9931-4f8b-8e07-d96b164aa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def sentinel_pixels(aerial_images, sentinel_images, sentinel_masks, centroids, sentinel_products, idx):\n",
    "\n",
    "    # Read in the aerial image. \n",
    "    with rasterio.open(aerial_images[idx], 'r') as f:\n",
    "        im = f.read().swapaxes(0, 2).swapaxes(0, 1)\n",
    "\n",
    "    # Determine the dates for all corresponding sentinel images\n",
    "    sentinel_dates = read_dates(sentinel_products[idx])\n",
    "    \n",
    "    #Read in the corresponding sentinel images\n",
    "    sen = np.load(sentinel_images[idx])[:,[2,1,0],:,:]/2000\n",
    "    \n",
    "    # Read in the corresponding cloud masks\n",
    "    clouds = np.load(sentinel_masks[idx])\n",
    "\n",
    "    dates_to_keep = filter_dates(sen, clouds)\n",
    "    print(len(dates_to_keep))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    sen_spatch = sen[:, centroids[idx][0]-int(20):centroids[idx][0]+int(20),centroids[idx][1]-int(20):centroids[idx][1]+int(20)]\n",
    "    print(centroids[idx])\n",
    "\n",
    "    return im\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fe087-4103-4011-9889-61be77f23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,38):\n",
    "    im = sentinel_pixels(train_aerial_images, train_sentinel_images, train_sentinel_masks, train_centroids, train_sentinel_products, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d78bc-4c6a-4c63-9f29-315f4a04df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = sentinel_pixels(train_aerial_images, train_sentinel_images, train_sentinel_masks, train_centroids, train_sentinel_products, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7d1a-3332-4dad-a604-1f8e3b9f3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "67-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54ec4f-9e05-4cb3-8821-a64a52f9f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075d917-eb62-443d-9c1c-1c187419db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a4104-32d9-4352-a7cb-9155b7af979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6a632-3e25-4370-9e5a-6b7ae2d0d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef17505-300d-469d-91a3-f9a4faeca0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(np.squeeze(sen[idx,:,:,:]), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadec0d-fa58-47b8-a52f-a48274d233e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(clouds[idx,1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271628c2-3887-4c01-bd0f-60d09d3f2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the spectra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ee5e8-5a48-4576-b1ed-b2c20e3bd73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014f2eb-2f84-4888-8688-52d40d8be76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import hex2color\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4b8f3-58a8-46be-b9a6-5de8ccbe5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_colors = {\n",
    "1   : '#db0e9a',\n",
    "2   : '#938e7b',\n",
    "3   : '#f80c00',\n",
    "4   : '#a97101',\n",
    "5   : '#1553ae',\n",
    "6   : '#194a26',\n",
    "7   : '#46e483',\n",
    "8   : '#f3a60d',\n",
    "9   : '#660082',\n",
    "10  : '#55ff00',\n",
    "11  : '#fff30d',\n",
    "12  : '#e4df7c',\n",
    "13  : '#3de6eb',\n",
    "14  : '#ffffff',\n",
    "15  : '#8ab3a0',\n",
    "16  : '#6b714f',\n",
    "17  : '#c5dc42',\n",
    "18  : '#9999ff',\n",
    "19  : '#000000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c94690-fde6-4a68-b0c7-9c0bde3ceb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_color(arr_2d: np.ndarray, palette: dict = lut_colors) -> np.ndarray:\n",
    "    rgb_palette = {k: tuple(int(i * 255) for i in hex2color(v)) for k, v in palette.items()}\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "    for c, i in rgb_palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "    return arr_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751563f-ae11-4c3f-a755-d5d5b30f7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(images, masks, sentinel_imgs, centroid, palette=lut_colors, idx=0) -> None:\n",
    "    print(idx)\n",
    "    fig, axs = plt.subplots(nrows = 2, ncols = 3, figsize = (10, 6)); fig.subplots_adjust(wspace=0.0, hspace=0.15)\n",
    "    fig.patch.set_facecolor('black')\n",
    "\n",
    "    with rasterio.open(images[idx], 'r') as f:\n",
    "        im = f.read([1,2,3]).swapaxes(0, 2).swapaxes(0, 1)\n",
    "    with rasterio.open(masks[idx], 'r') as f:\n",
    "        mk = f.read([1])\n",
    "        mk = convert_to_color(mk[0], palette=palette)\n",
    "    \n",
    "    sen = np.load(sentinel_imgs[idx])[20,[2,1,0],:,:]/2000\n",
    "    offset = (0, 0)\n",
    "    sen_spatch = sen[:, centroid[idx][0]-int(20) + offset[0]:centroid[idx][0]+int(20) + offset[0],\n",
    "        centroid[idx][1]-int(20) + offset[1]:centroid[idx][1]+int(20) + offset[1]]\n",
    "    transform = T.CenterCrop(10)\n",
    "    sen_aerialpatch = transform(torch.as_tensor(np.expand_dims(sen_spatch, axis=0))).numpy()\n",
    "    sen = np.transpose(sen, (1,2,0))\n",
    "    sen_spatch = np.transpose(sen_spatch, (1,2,0))\n",
    "    sen_aerialpatch = np.transpose(sen_aerialpatch[0], (1,2,0))\n",
    "\n",
    "    #axs = axs if isinstance(axs[], np.ndarray) else [axs]\n",
    "    ax0 = axs[0][0] ; ax0.imshow(im);ax0.axis('off')\n",
    "    ax1 = axs[0][1] ; ax1.imshow(mk, interpolation='nearest') ;ax1.axis('off')\n",
    "    ax2 = axs[0][2] ; ax2.imshow(im); ax2.imshow(mk, interpolation='nearest', alpha=0.25); ax2.axis('off')\n",
    "    ax3 = axs[1][0] ; ax3.imshow(sen);ax3.axis('off')\n",
    "    ax4 = axs[1][1] ; ax4.imshow(sen_spatch);ax4.axis('off')\n",
    "    ax5 = axs[1][2] ; ax5.imshow(sen_aerialpatch);ax5.axis('off')\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rect = Rectangle((centroid[idx][1]-5.12, centroid[idx][0]-5.12), 10.24, 10.24, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax3.add_patch(rect)\n",
    "    rect = Rectangle((14.88, 14.88), 10.24, 10.24, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax4.add_patch(rect)\n",
    "    \n",
    "    ax0.set_title('RGB Image', size=12,fontweight=\"bold\",c='w')\n",
    "    ax1.set_title('Ground Truth Mask', size=12,fontweight=\"bold\",c='w')\n",
    "    ax2.set_title('Overlay Image & Mask', size=12,fontweight=\"bold\",c='w')\n",
    "    ax3.set_title('Sentinel super area', size=12,fontweight=\"bold\",c='w')\n",
    "    ax4.set_title('Sentinel super patch', size=12,fontweight=\"bold\",c='w')\n",
    "    ax5.set_title('Sentinel over the aerial patch', size=12,fontweight=\"bold\",c='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e5c25-efa8-43d9-96bb-bf7b22510ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample(train_aerial_images, train_labels, train_sentinel_images, train_centroids, idx=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e28eb-3451-427a-b047-5f44ea328957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image 2\n",
    "# Image 5\n",
    "# Image 9 - clear half pixel shift\n",
    "# Image 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016fb7b-7937-4be5-b042-aff0c65b8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-100, 100)\n",
    "\n",
    "#-int(20):int(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22fac1-ad3b-47ba-889c-25ff5861ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
