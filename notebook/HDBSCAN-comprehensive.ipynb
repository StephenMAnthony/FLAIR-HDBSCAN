{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595f8fb-3624-43d5-bf7a-f8cc1cc87716",
   "metadata": {
    "id": "f631ae2d-c169-461c-937a-3446a78abe79",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# <center><strong>HDSCAN Clustering of FLAIR Data</strong></center>\n",
    "## <center><strong>Comprehensive Analysis</strong></center>\n",
    "<br/>\n",
    "\n",
    "<br/><center>This notebook provides a comprehensive view of the exploratory project to test whether HDBSCAN clustering can serve as a replacement for manual annotation. As a comprehensive analysis, all visualizations and analyses are displayed here. </center>\n",
    "<br/> <br/> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7decbf-7788-49ea-9e90-8190b994792b",
   "metadata": {},
   "source": [
    "<hr style=\"height:1.5px;border-width:0;color:red;background-color:red\">    \n",
    "\n",
    "# <center><font color='red'>PART-1: Data visualization with the toy dataset training data</font></center>\n",
    "\n",
    "Note, that this project uses the FLAIR #2 dataset, a publicly available dataset. A reference implementation (including a baseline model) is available in a GitHub repository. The baseline model uses a two-branch architecture integrating a U-Net with a pre-trained ResNet34 encoder and a U-TAE encompassing a temporal self-attention encoder. This project experiments with an alternative data analysis technique, HDBSCAN. As such, this notebook does not use the baseline model. Part 1 uses the reference implementation with minor modifications to load and display the unanalyzed data. The novel portion of this project starts at Part 2. \n",
    "\n",
    "Links\n",
    "Datapaper: https://arxiv.org/pdf/2305.14467.pdf\n",
    "Dataset link: https://ignf.github.io/FLAIR/#FLAIR2\n",
    "Reference Source code link: https://github.com/IGNF/FLAIR-2/tree/main\n",
    "Challenge page: https://codalab.lisn.upsaclay.fr/competitions/13447\n",
    "\n",
    "<p dir=\"auto\">Citation required when using the FLAIR #2 dataset:</p>\n",
    "<div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{ign2023flair2,\n",
    "      title={FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery}, \n",
    "      author={Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos},\n",
    "      year={2023},\n",
    "      booktitle={Advances in Neural Information Processing Systems (NeurIPS) 2023},\n",
    "      doi={https://doi.org/10.48550/arXiv.2310.13336},\n",
    "}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">ign2023flair2</span>,\n",
    "      <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery<span class=\"pl-pds\">}</span></span>, \n",
    "      <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Advances in Neural Information Processing Systems (NeurIPS) 2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">doi</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://doi.org/10.48550/arXiv.2310.13336<span class=\"pl-pds\">}</span></span>,\n",
    "}</pre></div>\n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a551697-f7df-4079-9719-d658971e8a34",
   "metadata": {},
   "source": [
    "Handle all the generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012f4c2-c578-4206-93ff-fe2f8ed0dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38dda1-76dd-4409-a682-a4866f1419ed",
   "metadata": {},
   "source": [
    "Import code from or based upon the FLAIR-2 reference implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc9829-c35d-42b0-b6f2-8c92b51748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAIR_path = join(Path.cwd().parents[0],'FLAIR-code/src')\n",
    "if FLAIR_path not in sys.path:\n",
    "    sys.path.append(FLAIR_path)\n",
    "\n",
    "from data_display import (display_nomenclature,\n",
    "                            display_samples, \n",
    "                            display_time_serie,\n",
    "                            display_all_with_semantic_class, \n",
    "                            display_all, \n",
    "                            read_dates, \n",
    "                            filter_dates)\n",
    "from load_data import load_data\n",
    "from FusedDataset import FusedDataset\n",
    "from calc_miou import calc_miou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefa9fa-4954-48ce-adaa-4d661b860f3d",
   "metadata": {
    "id": "88a3b469-b677-4aa2-826e-c00e4476805b",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Nomenclatures</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "The predefined semantic land-cover classes used in the FLAIR #2 datatset. <font color='#90c149'>Two nomenclatures are available </font> : \n",
    "<ul>\n",
    "    <li>the <strong><font color='#90c149'>full nomenclature</font></strong> corresponds to the semantic classes used by experts in photo-interpretation to label the pixels of the ground-truth images.</li>\n",
    "    <li>the <font color='#90c149'><b>main (baseline) nomenclature</b></font> is a simplified version of the full nomenclature. It regroups (into the class 'other') classes that are either strongly under-represented or irrelevant to this challenge.</li>\n",
    "</ul>        \n",
    "See the associated datapaper (https://arxiv.org/pdf/2305.14467.pdf) for additionnal details on these nomenclatures.<br/><br/>\n",
    "\n",
    "<font color='#90c149'>Note:</font> For this project, the reduced nomenclature is used. <br/><hr><br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df11640-4b34-4dcd-badd-bb5324857e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nomenclature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ebbdb-2eda-4d14-85ff-8d5a30542764",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Load Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Use reference code to create lists containing the paths to the input images (`images`) and supervision masks (`masks`) files of the dataset.<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214019e8-6f88-438b-ab7f-5a62c3cc8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/app/FLAIR-HDBSCAN/flair-2-config.yml\" \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Creation of the train, val, and test dictionaries with the data file paths\n",
    "# Note that due to using the toy dataset we assign 100% of the data for training. \n",
    "# While not best practice for machine learning, for the toy dataset when using the stock code and less than 100%, issues randomly arise when using the reference loader\n",
    "# as the random selection can result in some semantic classes not being represented. If the full FLAIR #2 dataset were employed, validation data should be separate. \n",
    "# Due to size limitations on HDBSCAN, for actual training we downsample to ~1 % of the training data for training and use 100% of the training data for fitting. \n",
    "d_train, d_val, d_test = load_data(config, val_percent=1)\n",
    "\n",
    "# Convert to torch Datasets\n",
    "train_dataset = FusedDataset(dict_files=d_train, config=config)\n",
    "valid_dataset = FusedDataset(dict_files=d_val, config=config)\n",
    "test_dataset = FusedDataset(dict_files=d_val, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd4398-7bba-4e38-baa3-fa0fad064ab0",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Load the training data. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe11eb-f9f9-423c-9419-666e43a3b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aerial_images = d_train[\"PATH_IMG\"]\n",
    "train_sentinel_images = d_train[\"PATH_SP_DATA\"]\n",
    "train_labels = d_train[\"PATH_LABELS\"]\n",
    "train_sentinel_masks = d_train[\"PATH_SP_MASKS\"] # Cloud masks\n",
    "train_sentinel_products = d_train[\"PATH_SP_DATES\"] # Needed to get the dates of the sentinel images\n",
    "train_centroids = d_train[\"SP_COORDS\"] # Position of the aerial image in the sentinel super area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dcb1f-29e4-4ca2-9e78-731275d07b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_aerial_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15b08c-f21c-4573-b3b8-9c4968aec83d",
   "metadata": {
    "id": "48cea476-3bd9-4de8-a562-2b325703d3b7",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Visualize Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Display some random samples of image and mask pairs. <font color='#90c149'>Re-run the cell bellow for a different image.</font> Here we also plot the Sentinel super area, super patch and patch. Even though the last one is not used in practice, it is shown to provide an idea of what the Sentinel data looks like. The red rectangle shows the extent of the RGB image inside the Sentinel image. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd677cc2-a9ef-4fad-985d-7c10de75ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(train_aerial_images, train_labels, train_sentinel_images, train_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db736c9a-2e0a-4b53-ac11-f7324f13b242",
   "metadata": {},
   "source": [
    "<br/><hr>\n",
    "We can also plot a few images from sentinel time series along with the acquisition date. Note that some dates may have extensive cloud coverage.\n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a0312-3563-4240-888b-5460280a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_time_serie(train_sentinel_images, train_sentinel_masks, train_sentinel_products, nb_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b01d9-b588-465c-8fdd-e768d632c005",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<br/><hr>\n",
    "\n",
    "Next let's have a closer look at some specific semantic class.<br/> By setting `semantic_class` to a class number (*e.g.*, `semantic_class`=1 for building or `semantic_class`=5 for water) we can visualize the images containing pixels of this specific class. (the full nomenclature is be used.)<br/>\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0c4f2-bc08-45dc-9106-ff6cd97e4c90",
   "metadata": {
    "id": "9ce3fcb7-8c9b-4207-bdf5-1227ba6600f0"
   },
   "outputs": [],
   "source": [
    "display_all_with_semantic_class(train_aerial_images, train_labels, semantic_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168d0ff-34f7-4589-a6ad-5583c4efa78a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "We can directly display all images.<br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893f2e8-8fd5-4e27-886e-e0e32daab3e1",
   "metadata": {
    "id": "a3b7d91f-3409-471a-b40c-8ca98be09a3f"
   },
   "outputs": [],
   "source": [
    "display_all(train_aerial_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69191fc-1f63-4512-acd8-7353a2fb252e",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-2: Naive Implementations </font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "In this section, I calculate performance metrics for two naive implementations:<br>\n",
    "1) Randomly assign each pixel to one of the 13 semantic classes, with uniform distribution across the classes. \n",
    "2) Randomly assign each pixel to one of the 13 semantic classes, with the probability of being assigned to a class equal to the prevalence of that class. \n",
    "\n",
    "<br/> \n",
    "Note, all code imported from this point onward or in this notebook was developed specifically for this project. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a61630-2108-4526-ae8b-71de38458183",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = join(Path.cwd().parents[0],'code')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e232d-e6e1-455d-8796-6702e52e20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import naive\n",
    "from naive import naive_clustering\n",
    "import display\n",
    "from display import display_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82553855-b2ff-4067-90a9-df77c82ee06a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Example of convenient code allowing reloading of a function. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8b6e-93c8-43f7-b1d6-562951f40a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(display)\n",
    "from display import display_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd5d24-7d25-4b4b-b775-c3b18f4d4a3c",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Generate confusion matrices for the naive implementations. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e07ff-9ba7-49aa-a0e4-ab024313aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = naive_clustering(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac9ea9-8abb-464b-9fe6-5ce44a8f47cd",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for naive implementation #1, uniform distribution. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0dd0f-ba62-4cb8-ba68-c9a7c64e7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['true_classes'], predictions_dict['random_classes'], config, 'naive uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2445d-1b3b-4a59-8cb1-50f0726e4c3a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for naive implementation #2, distribution with matched prevalence. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e8a53-a191-46e3-adf5-f7bf08f86c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['true_classes'], predictions_dict['permuted_classes'], config, 'naive prevalence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e86ad-47a9-4ed5-93c7-444a58cdd07b",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-3: Visualizing the Aerial Data</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Visualizations of the aerial data from the training data. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b13476-d12f-4e88-a891-2b8a99fa42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import display\n",
    "reload(display)\n",
    "from display import box_whisker_by_class\n",
    "from display import class_distributions\n",
    "\n",
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import extract_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274c974-c500-4216-8c60-a6126509084a",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<hr>\n",
    "Display the class distribution of pixels in the training data. \n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e72ba-3cb1-4bcf-803c-5633d2cd0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = class_distributions(train_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd07407-df78-45ff-bdeb-80cadc369660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = extract_spectra(train_dataset, config, downsample=True, no_other=True, scale_by_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bca95d-6b20-4f2e-99a2-cd652e9321d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8203ea-7ba0-4533-9152-bf2a9d6f7db4",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<hr>\n",
    "Display the distribution of channel values by semantic classes.<br/> Setting third input (`channel`) to a channel number (*e.g.*, Blue=1, Green=2, Red=3, NIR=4, ..., Elevation=15) displays a box and whisker plot. The box extends from the data's first quartile (Q1) to the third quartile (Q3), where the orange line represents the median. The interquartile distance (IQR) is between Q1 and Q3 (Q3 - Q1). Data points below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are classified as outliers or fliers; such points are displayed individually with circles. Whiskers extend from the box in each direction to the farthest data point which is not an outlier or flier. \n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d4843-9da6-41ee-a98a-aaedea7c1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_whisker_by_class(dataframe, config, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ca57a-be05-40c0-9360-86e06d64fa98",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-4: K-nearest neighbor analysis of aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Here, I train a k-nearest neighbor classifier on aerial imagery. The FLAIR #2 toy dataset is employed, which previously was split into training and test datasets. While best practice is typically to employ training, validation, and test datasets, when working with the toy dataset the random subsetting of the training data into training and validation caused issues as not all classes were always represented in all datasets. Additionally, HDBSCAN (run later) was found to have issues scaling to 1 million pixels. Therefore, rather than randomly assigning some of the 512x512 pixel patches to train and others to validation, we do not assign any patches to validation. Instead, we downsample the training data by a factor of 10 in each dimension before training, effectively using 1% of the training data to train. Validation is performed on the complete training data, of which 99% was not used for training. <br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59677312-cc9f-4dfe-9146-2b7bddc4f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import train_and_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f9320-16d3-4823-8fc6-6eb072471399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_model_and_predictions = train_and_validate_model(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d782943-1153-4482-b415-653485d77e2f",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the aerial spectra. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dfd9b-2f22-4405-8c84-72d6563e39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_model_and_predictions['true_classes'], knn_model_and_predictions['predicted_classes'], config, 'knn validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8bea8-ad36-472b-abb7-ea94639afe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_model_and_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3971856-220c-4bd9-8cf5-25f280b460f5",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "##### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-5: Spectral Normalization</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Spectral analysis is sometimes improved by separating the spectral profile from the intensity. Normalize all the spectra and add an additional feature corresponding to the intensity. One reason this can be useful is that due to a combination of sun angle and/or off-nadir imaging, there might be shadows. Shadows will generally have a similar spectral shape but different intensity. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6d3b1-bcec-44ec-b033-e1de8612a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import display\n",
    "reload(display)\n",
    "from display import display_normalization_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630f8ff-52fd-446c-9e0b-f772d30f0eca",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "The two plots below show scatter plot of the distribution of values of the two spectral components without and with spectral normalization. \n",
    "The input channel values (e.g., 'channel1', 'channel2') can be varied using the channel numbers (*e.g.*, Blue=1, Green=2, Red=3, NIR=4, ..., Elevation=15) \n",
    "The raw spectra show a strong correlation between the red and green channels, but comparison with the normalized spectra indicates that much of that correlation is simply a similar dependence upon intensity. When the spectra are normalized, additional grouping and altered distribution is seen. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b114d8-bfa7-45e5-ac00-a27c34f1bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_normalization_scatter(train_dataset, config, channel1=2, channel2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f77c09-e13a-47f5-8eb2-33d11256596e",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-6: K-nearest neighbor analysis of normalized aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of spectral normalization on the k-nearest neighbor analysis. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d5b0d-0a14-4b76-ab75-d13a83953029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_normalized = train_and_validate_model(train_dataset, config, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac497e8e-85bd-4b69-b85b-a5a6143b534e",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the normalized aerial spectra with appended intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd239c-5ae6-4a97-8a59-1821c4e3d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_normalized['true_classes'], knn_normalized['predicted_classes'], config, 'knn validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a5778-1531-43fe-b154-d6d3922e045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13b341-ff99-4894-b2a8-28607e85500e",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "These results indicate that normalizing the spectra and appending the intensity do not improve the classification. One possibility is that appending the intensity is the issue. Try without that. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1dd5f-1285-4679-8888-fa2cc73094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_normalized_no_intensity = train_and_validate_model(train_dataset, config, scale_by_intensity=True, append_intensity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34b5f9-758a-4803-86ca-f707acd153b5",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the normalized aerial spectra with appended intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5ad97-7a9a-48b7-a946-bd23eabcd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_normalized_no_intensity['true_classes'], knn_normalized_no_intensity['predicted_classes'], config, 'knn normalized spectra no intensity validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ece829-844f-4362-9bbf-f13c714324ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_normalized_no_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaaa18-a56c-4ce2-8eda-ceb90ac1eab7",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Neither of these showed improvement over the baseline k-nearest neighbor classifier. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc07ae-ef3b-4d61-8b0c-f08eb89c2e57",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-7: HDBSCAN analysis of raw aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "Here, HDBSCAN is applied to the raw aerial imagery to determine clusters. Since HDBSCAN is a density-aware clustering algorithm, some samples will be classified as outliers and not assigned to a cluster. To ensure that the HDSCAN analysis does not simply fit only the easiest-to-classify samples and to offer a fair comparison to standard k-nearest neighbors, processing with HDBSCAN needed to be extended to assign a cluster or class label to all samples. Therefore, analogously to the prior k-nearest neighbor analysis, a k-nearest neighbor model was trained using some pixels and their labels and then applied to all the pixels. For standard k-nearest neighbors, the model was trained using the ground truth classes and the 1% downsampled training data. For HDBSCAN, for each cluster label, the most commonly represented class was designated as the class label which was then used to train the k-nearest neighbor model. Additionally, only pixels assigned to a cluster were used to train the k-nearest neighbor model. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a288e76-d227-42f2-adb9-7871e8f879ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_model_and_predictions = train_and_validate_model(train_dataset, config, use_hdbscan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790b076-0fc0-4077-9624-5df274c59c97",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "As hypothesized, HDBSCAN found a workable number of clusters where clusters tend to be sub-classes of the FLAIR semantic classes. The accuracy of mapping clusters to individual semantic classes is seen to be quite high. \n",
    "<br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb0dae-b75c-4183-9668-8ce259cd6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_model_and_predictions['true_classes'], hdbscan_model_and_predictions['predicted_classes'], config, 'HDBSCAN Validation cluster size 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190435c-74d3-417f-bc9d-dd0135dff4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_model_and_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b33323-acc6-4593-b902-f3231b431909",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Unsurprisingly, training the k-nearest neighbor model with fewer pixels and predicted labels rather than ground truth labels results in a lower metric than simply using k-nearest neighbors directly. However, k-nearest neighbors is a supervised machine learning method requiring expensive manual annotation. HDBSCAN offers excellent performance when considering that it is an unsupervised machine learning technique. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ba40f-14d0-4983-832e-76cb5064c893",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-8: HDBSCAN analysis of normalized aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of spectral normalization on HDBSCAN analysis. While spectral normalization was not beneficial to k-nearest neighboring, it might help HDBSCAN clustering based upon the observed changes when visualizing the distributions after spectral normalization. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ab2ff-a102-4d79-989a-516200f33219",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_normalized = train_and_validate_model(train_dataset, config, use_hdbscan=True, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579e707-22c5-4054-b1b3-90c8dfcbef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_normalized['true_classes'], hdbscan_normalized['predicted_classes'], config, 'HDBSCAN with Normalized Spectra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63c8cb-2b22-4965-9621-fa03d6f779d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035d44a-7f47-4e6a-b544-883ba3ad82b8",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Worse performance than the baseline HDBSCAN analysis. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44389ecd-254f-40f5-a9a0-0ea91e002aef",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-9: Analysis of robust scaled aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of robust scaling on HDBSCAN and KNN analysis. Robust scaling is data preprocessing to normalize the data, specifically centering each component to its median and scaling each component according to its interquartile range. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37597041-81e1-4d4f-ba6c-07e4678f0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_robust = train_and_validate_model(train_dataset, config, use_hdbscan=False, robust_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07225346-c3d3-47b6-9d48-ee8b9c43016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_robust['true_classes'], knn_robust['predicted_classes'], config, 'KNN with robust scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021e4c8-6ec2-4c36-894a-bfe2dc1196ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402df38-0e62-42ed-88bc-2c32acb5159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_robust = train_and_validate_model(train_dataset, config, use_hdbscan=True, robust_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267ac7c-86f0-4856-80c1-8b2d6186406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_robust['true_classes'], hdbscan_robust['predicted_classes'], config, 'HDBSCAN Validation cluster size 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817a173-a4c1-4f2d-b1cc-c90a264523c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2828c9c-b8c4-442f-b8fe-e58c97338aee",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-10: Analysis of robust scaled aerial imagery with spectral normalization</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of combining robust scaling and spectral normalization on HDBSCAN and KNN analysis. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e78f7-1352-46dd-ba75-e5f5116ab6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_robust = train_and_validate_model(train_dataset, config, use_hdbscan=False, robust_scale=True, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ecb28-dc11-4708-8cf4-1d3f0a27feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_robust['true_classes'], knn_robust['predicted_classes'], config, 'KNN with robust scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0049bf3-c2d4-4263-a5ba-20599e801d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c8c1c-27b7-404f-a563-f4976b6715fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_robust = train_and_validate_model(train_dataset, config, use_hdbscan=True, robust_scale=True, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f15e9e-d738-48cd-a7a8-394d0efd89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_robust['true_classes'], hdbscan_robust['predicted_classes'], config, 'HDBSCAN Validation cluster size 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236541d-ebf8-4140-9888-d49aa33071bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc460b0-4253-4529-a428-9696a731e629",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-11: Data Fusion - Satellite & Aerial Imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of data fusion of satellite imagery with aerial imagery.\n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880b7cc-744e-499b-852a-935aee43fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_satellite = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=False, robust_scale=False, scale_by_intensity=False, append_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cf60b-b340-4172-bf57-62c1f61b55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_satellite['true_classes'], knn_satellite['predicted_classes'], config, 'KNN Aerial and Satellite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4536-b07e-484e-b2e3-185d3f9b81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be466a64-e1ba-4a7e-99fb-73c2c82db1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_satellite = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=True, robust_scale=False, scale_by_intensity=False, append_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebc234-2c6c-4e61-af27-7f73ee19bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_satellite['true_classes'], hdbscan_satellite['predicted_classes'], config, 'HDBSCAN Aerial and Satellite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71645e4e-6c52-48b4-91b3-4794f7739af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_satellite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61d367-e080-46e7-9873-7ee69c9467aa",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-12: Data Fusion - Spectral Normalization</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of data fusion of satellite imagery with aerial imagery combined with spectral normalization\n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f88f0-44dd-41b7-9cfc-8f9e1ae45ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_fusion_normalized = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=False, robust_scale=False, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0d0b1-cc93-4e24-97b5-cc291a0f3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_fusion_normalized['true_classes'], knn_fusion_normalized['predicted_classes'], config, 'KNN Fusion w/ Spectral Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e55744-4a50-4859-b4ba-dcc2448ae56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_fusion_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8fc96-1519-4557-bdd0-5feab4096481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_fusion_normalized = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=True, robust_scale=False, scale_by_intensity=True, append_intensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40179b-51bd-439a-996d-faa16be5680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_fusion_normalized['true_classes'], hdbscan_fusion_normalized['predicted_classes'], config, 'HDBSCAN Fusion w/ Spectral Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628b94a-d911-4b0c-a00e-eb15d7511c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_fusion_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b979c4-7924-4b11-978b-3ce39e5ec770",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-13: Data Fusion - Robust</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section shows the effects of data fusion of satellite imagery with aerial imagery combined with spectral normalization\n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ece3ea-a14d-42ff-ac83-7be16e5d0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_fusion_robust = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=False, robust_scale=True, scale_by_intensity=False, append_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d9ad3-eef0-44d7-b360-c12079a9e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_fusion_robust['true_classes'], knn_fusion_robust['predicted_classes'], config, 'KNN Fusion Robust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb9c52-d0d7-42ec-84e1-759ae37c8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f knn_fusion_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51ed1f-fcb5-48b4-b00d-74f6c2531582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_fusion_robust = train_and_validate_model(train_dataset, config, use_satellite=True, use_hdbscan=True, robust_scale=True, scale_by_intensity=False, append_intensity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2485f-09dc-4b6a-bb57-e09d0f13c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_fusion_robust['true_classes'], hdbscan_fusion_robust['predicted_classes'], config, 'HDBSCAN Fusion Robust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a6708-5105-4a57-956b-f6cb31cec609",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f hdbscan_fusion_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d73f1f-70a4-4358-9b38-107ce997ce5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9fbff-3221-4559-9cd9-f2f9a8725a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
