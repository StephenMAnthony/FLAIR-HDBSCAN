{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595f8fb-3624-43d5-bf7a-f8cc1cc87716",
   "metadata": {
    "id": "f631ae2d-c169-461c-937a-3446a78abe79",
    "tags": []
   },
   "source": [
    "# <center><strong>HDSCAN Clustering of FLAIR Data</strong></center>\n",
    "## <center><strong>Comprehensive Analysis</strong></center>\n",
    "<br/>\n",
    "\n",
    "<br/><center>This notebook provides a comprehensive view of the exploratory project to test whether HDBSCAN clustering can serve as a replacement for manual annotation. As a comprehensive analysis, all visualizations and analyses are displayed here. </center>\n",
    "<br/> <br/> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7decbf-7788-49ea-9e90-8190b994792b",
   "metadata": {},
   "source": [
    "<hr style=\"height:1.5px;border-width:0;color:red;background-color:red\">    \n",
    "\n",
    "# <font color='red'>PART-1: Data visualization with the toy dataset training data</font>\n",
    "\n",
    "Note, that this project uses the FLAIR #2 dataset, a publicly available dataset. A reference implementation (including a baseline model) is available in a GitHub repository. The baseline model uses a two-branch architecture integrating a U-Net with a pre-trained ResNet34 encoder and a U-TAE encompassing a temporal self-attention encoder. This project experiments with an alternative data analysis technique, HDBSCAN. As such, this notebook does not use the baseline model. Part 1 uses the reference implementation with minor modifications to load and display the unanalyzed data. The novel portion of this project starts at Part 2. \n",
    "\n",
    "Links\n",
    "Datapaper: https://arxiv.org/pdf/2305.14467.pdf\n",
    "Dataset link: https://ignf.github.io/FLAIR/#FLAIR2\n",
    "Reference Source code link: https://github.com/IGNF/FLAIR-2/tree/main\n",
    "Challenge page: https://codalab.lisn.upsaclay.fr/competitions/13447\n",
    "\n",
    "<p dir=\"auto\">Citation required when using the FLAIR #2 dataset:</p>\n",
    "<div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{ign2023flair2,\n",
    "      title={FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery}, \n",
    "      author={Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos},\n",
    "      year={2023},\n",
    "      booktitle={Advances in Neural Information Processing Systems (NeurIPS) 2023},\n",
    "      doi={https://doi.org/10.48550/arXiv.2310.13336},\n",
    "}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">ign2023flair2</span>,\n",
    "      <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery<span class=\"pl-pds\">}</span></span>, \n",
    "      <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Advances in Neural Information Processing Systems (NeurIPS) 2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">doi</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://doi.org/10.48550/arXiv.2310.13336<span class=\"pl-pds\">}</span></span>,\n",
    "}</pre></div>\n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a551697-f7df-4079-9719-d658971e8a34",
   "metadata": {},
   "source": [
    "Handle all the generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012f4c2-c578-4206-93ff-fe2f8ed0dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38dda1-76dd-4409-a682-a4866f1419ed",
   "metadata": {},
   "source": [
    "Import code from or based upon the FLAIR-2 reference implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc9829-c35d-42b0-b6f2-8c92b51748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAIR_path = join(Path.cwd().parents[0],'FLAIR-code/src')\n",
    "if FLAIR_path not in sys.path:\n",
    "    sys.path.append(FLAIR_path)\n",
    "\n",
    "from data_display import (display_nomenclature,\n",
    "                            display_samples, \n",
    "                            display_time_serie,\n",
    "                            display_all_with_semantic_class, \n",
    "                            display_all, \n",
    "                            read_dates, \n",
    "                            filter_dates)\n",
    "from load_data import load_data\n",
    "from FusedDataset import FusedDataset\n",
    "from calc_miou import calc_miou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefa9fa-4954-48ce-adaa-4d661b860f3d",
   "metadata": {
    "id": "88a3b469-b677-4aa2-826e-c00e4476805b",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Nomenclatures</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "The predefined semantic land-cover classes used in the FLAIR #2 datatset. <font color='#90c149'>Two nomenclatures are available </font> : \n",
    "<ul>\n",
    "    <li>the <strong><font color='#90c149'>full nomenclature</font></strong> corresponds to the semantic classes used by experts in photo-interpretation to label the pixels of the ground-truth images.</li>\n",
    "    <li>the <font color='#90c149'><b>main (baseline) nomenclature</b></font> is a simplified version of the full nomenclature. It regroups (into the class 'other') classes that are either strongly under-represented or irrelevant to this challenge.</li>\n",
    "</ul>        \n",
    "See the associated datapaper (https://arxiv.org/pdf/2305.14467.pdf) for additionnal details on these nomenclatures.<br/><br/>\n",
    "\n",
    "<font color='#90c149'>Note:</font> For this project, the reduced nomenclature is used. <br/><hr><br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df11640-4b34-4dcd-badd-bb5324857e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nomenclature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ebbdb-2eda-4d14-85ff-8d5a30542764",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Load Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Use reference code to create lists containing the paths to the input images (`images`) and supervision masks (`masks`) files of the dataset.<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214019e8-6f88-438b-ab7f-5a62c3cc8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/app/FLAIR-HDBSCAN/flair-2-config.yml\" \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Creation of the train, val, and test dictionaries with the data file paths\n",
    "# Note that due to using the toy dataset we assign 100% of the data for training. \n",
    "# While not best practice for machine learning, for the toy dataset when using the stock code and less than 100%, issues randomly arise when using the reference loader\n",
    "# as the random selection can result in some semantic classes not being represented. If the full FLAIR #2 dataset were employed, validation data should be separate. \n",
    "# Due to size limitations on HDBSCAN, for actual training we downsample to ~1 % of the training data for training and use 100% of the training data for fitting. \n",
    "d_train, d_val, d_test = load_data(config, val_percent=1)\n",
    "\n",
    "# Convert to torch Datasets\n",
    "train_dataset = FusedDataset(dict_files=d_train, config=config)\n",
    "valid_dataset = FusedDataset(dict_files=d_val, config=config)\n",
    "test_dataset = FusedDataset(dict_files=d_val, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd4398-7bba-4e38-baa3-fa0fad064ab0",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Load the training data. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe11eb-f9f9-423c-9419-666e43a3b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aerial_images = d_train[\"PATH_IMG\"]\n",
    "train_sentinel_images = d_train[\"PATH_SP_DATA\"]\n",
    "train_labels = d_train[\"PATH_LABELS\"]\n",
    "train_sentinel_masks = d_train[\"PATH_SP_MASKS\"] # Cloud masks\n",
    "train_sentinel_products = d_train[\"PATH_SP_DATES\"] # Needed to get the dates of the sentinel images\n",
    "train_centroids = d_train[\"SP_COORDS\"] # Position of the aerial image in the sentinel super area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15b08c-f21c-4573-b3b8-9c4968aec83d",
   "metadata": {
    "id": "48cea476-3bd9-4de8-a562-2b325703d3b7",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Visualize Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Display some random samples of image and mask pairs. <font color='#90c149'>Re-run the cell bellow for a different image.</font> Here we also plot the Sentinel super area, super patch and patch. Even though the last one is not used in practice, it is shown to provide an idea of what the Sentinel data looks like. The red rectangle shows the extent of the RGB image inside the Sentinel image. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd677cc2-a9ef-4fad-985d-7c10de75ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(train_aerial_images, train_labels, train_sentinel_images, train_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db736c9a-2e0a-4b53-ac11-f7324f13b242",
   "metadata": {},
   "source": [
    "<br/><hr>\n",
    "We can also plot a few images from sentinel time series along with the acquisition date. Note that some dates may have extensive cloud coverage.\n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a0312-3563-4240-888b-5460280a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_time_serie(train_sentinel_images, train_sentinel_masks, train_sentinel_products, nb_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b01d9-b588-465c-8fdd-e768d632c005",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<br/><hr>\n",
    "\n",
    "Next let's have a closer look at some specific semantic class.<br/> By setting `semantic_class` to a class number (*e.g.*, `semantic_class`=1 for building or `semantic_class`=5 for water) we can visualize the images containing pixels of this specific class. (the full nomenclature is be used.)<br/>\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0c4f2-bc08-45dc-9106-ff6cd97e4c90",
   "metadata": {
    "id": "9ce3fcb7-8c9b-4207-bdf5-1227ba6600f0"
   },
   "outputs": [],
   "source": [
    "display_all_with_semantic_class(train_aerial_images, train_labels, semantic_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168d0ff-34f7-4589-a6ad-5583c4efa78a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "We can directly display all images.<br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893f2e8-8fd5-4e27-886e-e0e32daab3e1",
   "metadata": {
    "id": "a3b7d91f-3409-471a-b40c-8ca98be09a3f"
   },
   "outputs": [],
   "source": [
    "display_all(train_aerial_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69191fc-1f63-4512-acd8-7353a2fb252e",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-2: Naive Implementations </font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "In this section, I calculate performance metrics for two naive implementations:<br>\n",
    "1) Randomly assign each pixel to one of the 13 semantic classes, with uniform distribution across the classes. \n",
    "2) Randomly assign each pixel to one of the 13 semantic classes, with the probability of being assigned to a class equal to the prevalence of that class. \n",
    "\n",
    "<br/> \n",
    "Note, all code imported from this point onward or in this notebook was developed specifically for this project. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a61630-2108-4526-ae8b-71de38458183",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = join(Path.cwd().parents[0],'code')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e232d-e6e1-455d-8796-6702e52e20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import naive\n",
    "from naive import naive_clustering\n",
    "import display\n",
    "from display import display_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82553855-b2ff-4067-90a9-df77c82ee06a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Example of convenient code allowing reloading of a function. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8b6e-93c8-43f7-b1d6-562951f40a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(display)\n",
    "from display import display_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd5d24-7d25-4b4b-b775-c3b18f4d4a3c",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Generate confusion matrices for the naive implementations. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e07ff-9ba7-49aa-a0e4-ab024313aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = naive_clustering(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac9ea9-8abb-464b-9fe6-5ce44a8f47cd",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for naive implementation #1, uniform distribution. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0dd0f-ba62-4cb8-ba68-c9a7c64e7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['true_classes'], predictions_dict['random_classes'], config, 'naive uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2445d-1b3b-4a59-8cb1-50f0726e4c3a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for naive implementation #2, distribution with matched prevalence. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e8a53-a191-46e3-adf5-f7bf08f86c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['true_classes'], predictions_dict['permuted_classes'], config, 'naive prevalence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e86ad-47a9-4ed5-93c7-444a58cdd07b",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-3: Visualizing the Aerial Data</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Visualizations of the aerial data from the training data. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b13476-d12f-4e88-a891-2b8a99fa42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import display\n",
    "reload(display)\n",
    "from display import box_whisker_by_class\n",
    "\n",
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import extract_aerial_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd07407-df78-45ff-bdeb-80cadc369660",
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial_data = extract_aerial_spectra(train_dataset, config, downsample=True, no_other=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea9865-67a6-4f4a-b5af-274d58e0d3c2",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<br/><hr>\n",
    "\n",
    "Display the distribution of channel values by semantic classes.<br/> Setting third input (`channel`) to a channel number (*e.g.*, Blue=1, Green=2, Red=3, NIR=4, Elevation=5) displays a box and whisker plot. The box extends from the data's first quartile (Q1) to the third quartile (Q3), where the orange line represents the median. The interquartile distance (IQR) is between Q1 and Q3 (Q3 - Q1). Data points below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are classified as outliers or fliers; such points are displayed individually with circles. Whiskers extend from the box in each direction to the farthest data point which is not an outlier or flier. \n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d4843-9da6-41ee-a98a-aaedea7c1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_whisker_by_class(aerial_data, config, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ca57a-be05-40c0-9360-86e06d64fa98",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-4: K-nearest neighbor analysis of aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Here, I train a k-nearest neighbor classifier on aerial imagery. The FLAIR #2 toy dataset is employed, which previously was split into training and test datasets. While best practice is typically to employ training, validation, and test datasets, when working with the toy dataset the random subsetting of the training data into training and validation caused issues as not all classes were always represented in all datasets. Additionally, HDBSCAN (run later) was found to have issues scaling to 1 million pixels. Therefore, rather than randomly assigning some of the 512x512 pixel patches to train and others to validation, we do not assign any patches to validation. Instead, we downsample the training data by a factor of 10 in each dimension before training, effectively using 1% of the training data to train. Validation is performed on the complete training data, of which 99% was not used for training. <br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59677312-cc9f-4dfe-9146-2b7bddc4f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import train_and_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f9320-16d3-4823-8fc6-6eb072471399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_model_and_predictions = train_and_validate_model(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d782943-1153-4482-b415-653485d77e2f",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the aerial spectra. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dfd9b-2f22-4405-8c84-72d6563e39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_model_and_predictions['true_classes'], knn_model_and_predictions['predicted_classes'], config, 'knn validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa5770-035e-45fa-ac6f-b951bd237602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9a8ed-0a9f-4f39-884a-83e25a072f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778cc9b-2f69-42d6-a2e6-15f08d9003fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d1b3c-6195-42d6-a6eb-a19e17e51c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3971856-220c-4bd9-8cf5-25f280b460f5",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "#### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-4: Spectral Normalization</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Spectral analysis may be improved by separating the spectral profile from the intensity. Normalize all the spectra and add an additional feature corresponding to the intensity. One reason this can be useful is that due to a combination of sun angle and/or off-nadir imaging, there might be shadows. Shadows will generally have a similar spectral shape but different intensity. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb2ae4-aac3-48ad-8c43-038e7f3accb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_spectra, labels = extract_spectra(train_dataset, config)\n",
    "normalized_spectra = normalize_spectra(original_spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630f8ff-52fd-446c-9e0b-f772d30f0eca",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "The plot below shows a scatter plot of the distribution of values of the first two raw spectral components. A strong correlation is observed between the values of these components, related to differences in intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc635c97-a94e-4de2-9fb5-0a462d3ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwds = {'alpha': 0.1, 's': 80, 'linewidths':0}\n",
    "plt.scatter(original_spectra[0,:], original_spectra[1,:], color='b', **plot_kwds)\n",
    "plt.xlabel(\"Spectral Component #1\")\n",
    "plt.ylabel(\"Spectral Component #2\")\n",
    "plt.title(\"Raw Spectra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff582a-05fb-44cf-93d2-f31a98949458",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "The plot below shows a scatter plot of the distribution of values of the first two spectral components after normalization. Normalization significantly reduces the correlation between these components. Separate clusters and lobes become much more apparent. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879f581-b16b-40a4-aa1c-96c9bf38f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwds = {'alpha': 0.1, 's': 80, 'linewidths':0}\n",
    "plt.scatter(normalized_spectra[0,:], normalized_spectra[1,:], color='b', **plot_kwds)\n",
    "plt.xlabel(\"Spectral Component #1\")\n",
    "plt.ylabel(\"Spectral Component #2\")\n",
    "plt.title(\"Normalized Spectra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0373cd2-1a63-419c-8a03-af67a196fb7e",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "The plot below shows a scatter plot of the distribution of values of the first two spectral components. A strong correlation is observed between the values of these components, related to differences in intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5145e7c-fdf3-4ef2-bcd2-93859b5fd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_normalized_model_and_predictions = train_and_validate_knn(train_dataset, config, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3389f1-0fbc-4c47-b575-8eb4a2e7b069",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the normalized aerial spectra with appended intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbdd6c-2b7e-4ba2-a078-6920ebf0aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_normalized_model_and_predictions['true_classes'], knn_normalized_model_and_predictions['predicted_classes'], config, 'knn normalized spectra validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13b341-ff99-4894-b2a8-28607e85500e",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "These results indicate that normalizing the spectra and appending the intensity do not improve the classification. One possibility is that appending the intensity is the issue. Try without that. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1dd5f-1285-4679-8888-fa2cc73094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_normalized_no_intensity_model_and_predictions = train_and_validate_knn(train_dataset, config, normalize=True, append_intensity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34b5f9-758a-4803-86ca-f707acd153b5",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for k-nearest neighbor classification on the normalized aerial spectra with appended intensity. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5ad97-7a9a-48b7-a946-bd23eabcd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_normalized_no_intensity_model_and_predictions['true_classes'], knn_normalized_no_intensity_model_and_predictions['predicted_classes'], config, 'knn normalized spectra no intensity validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaaa18-a56c-4ce2-8eda-ceb90ac1eab7",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Neither of these showed improvement over the baseline k-nearest neighbor classifier. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e43cd-12e4-403f-a67a-9fd7a98d2685",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "###### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-5: HDBSCAN analysis of aerial imagery</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Here, I apply HDBSCAN clustering to the aerial imagery training data from the FLAIR #2 toy dataset.<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213e4e0-15e5-49df-8ed3-d7a5fcb1ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64592aef-6c54-4e8a-b3b0-902f9e3815c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(classifier)\n",
    "from classifier import normalize_spectra\n",
    "from classifier import extract_spectra\n",
    "from classifier import assign_class_to_cluster\n",
    "from classifier import train_and_validate_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19678739-c8ac-4f15-8e00-6625be04b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_model_and_predictions = train_and_validate_hdbscan(train_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dd70d-6ae7-4dd2-a754-36273417ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_model_and_predictions['true_classes'], hdbscan_model_and_predictions['predicted_classes'], config, 'HDBSCAN Validation cluster size 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbb02c-ad85-42ec-b332-6a7ada249d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933a1a4-1076-4a31-971e-f961a5e9a368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24b98a-15c6-4b46-92da-d89ed51943da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7017109-8388-4f1a-94b7-1a634543e802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f072f-2098-43cf-94a9-d28767e9272b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe15a6a-13bd-4b46-8c8b-04d6fb258009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b7271-372f-46fb-a902-226469e8f945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412d0d8-0979-47fd-a1e4-00c83d9639fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93535c67-9931-4f8b-8e07-d96b164aa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "def sentinel_pixels(aerial_images, sentinel_images, sentinel_masks, centroids, sentinel_products, idx):\n",
    "\n",
    "    # Read in the aerial image. \n",
    "    with rasterio.open(aerial_images[idx], 'r') as f:\n",
    "        im = f.read().swapaxes(0, 2).swapaxes(0, 1)\n",
    "\n",
    "    # Determine the dates for all corresponding sentinel images\n",
    "    sentinel_dates = read_dates(sentinel_products[idx])\n",
    "    \n",
    "    #Read in the corresponding sentinel images\n",
    "    sen = np.load(sentinel_images[idx])[:,[2,1,0],:,:]/2000\n",
    "    \n",
    "    # Read in the corresponding cloud masks\n",
    "    clouds = np.load(sentinel_masks[idx])\n",
    "\n",
    "    dates_to_keep = filter_dates(sen, clouds)\n",
    "    print(len(dates_to_keep))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    sen_spatch = sen[:, centroids[idx][0]-int(20):centroids[idx][0]+int(20),centroids[idx][1]-int(20):centroids[idx][1]+int(20)]\n",
    "    print(centroids[idx])\n",
    "\n",
    "    return im\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fe087-4103-4011-9889-61be77f23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,38):\n",
    "    im = sentinel_pixels(train_aerial_images, train_sentinel_images, train_sentinel_masks, train_centroids, train_sentinel_products, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d78bc-4c6a-4c63-9f29-315f4a04df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = sentinel_pixels(train_aerial_images, train_sentinel_images, train_sentinel_masks, train_centroids, train_sentinel_products, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7d1a-3332-4dad-a604-1f8e3b9f3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "67-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54ec4f-9e05-4cb3-8821-a64a52f9f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075d917-eb62-443d-9c1c-1c187419db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a4104-32d9-4352-a7cb-9155b7af979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6a632-3e25-4370-9e5a-6b7ae2d0d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef17505-300d-469d-91a3-f9a4faeca0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(np.squeeze(sen[idx,:,:,:]), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadec0d-fa58-47b8-a52f-a48274d233e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(clouds[idx,1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271628c2-3887-4c01-bd0f-60d09d3f2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the spectra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ee5e8-5a48-4576-b1ed-b2c20e3bd73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014f2eb-2f84-4888-8688-52d40d8be76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import hex2color\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4b8f3-58a8-46be-b9a6-5de8ccbe5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_colors = {\n",
    "1   : '#db0e9a',\n",
    "2   : '#938e7b',\n",
    "3   : '#f80c00',\n",
    "4   : '#a97101',\n",
    "5   : '#1553ae',\n",
    "6   : '#194a26',\n",
    "7   : '#46e483',\n",
    "8   : '#f3a60d',\n",
    "9   : '#660082',\n",
    "10  : '#55ff00',\n",
    "11  : '#fff30d',\n",
    "12  : '#e4df7c',\n",
    "13  : '#3de6eb',\n",
    "14  : '#ffffff',\n",
    "15  : '#8ab3a0',\n",
    "16  : '#6b714f',\n",
    "17  : '#c5dc42',\n",
    "18  : '#9999ff',\n",
    "19  : '#000000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c94690-fde6-4a68-b0c7-9c0bde3ceb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_color(arr_2d: np.ndarray, palette: dict = lut_colors) -> np.ndarray:\n",
    "    rgb_palette = {k: tuple(int(i * 255) for i in hex2color(v)) for k, v in palette.items()}\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "    for c, i in rgb_palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "    return arr_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751563f-ae11-4c3f-a755-d5d5b30f7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(images, masks, sentinel_imgs, centroid, palette=lut_colors, idx=0) -> None:\n",
    "    print(idx)\n",
    "    fig, axs = plt.subplots(nrows = 2, ncols = 3, figsize = (10, 6)); fig.subplots_adjust(wspace=0.0, hspace=0.15)\n",
    "    fig.patch.set_facecolor('black')\n",
    "\n",
    "    with rasterio.open(images[idx], 'r') as f:\n",
    "        im = f.read([1,2,3]).swapaxes(0, 2).swapaxes(0, 1)\n",
    "    with rasterio.open(masks[idx], 'r') as f:\n",
    "        mk = f.read([1])\n",
    "        mk = convert_to_color(mk[0], palette=palette)\n",
    "    \n",
    "    sen = np.load(sentinel_imgs[idx])[20,[2,1,0],:,:]/2000\n",
    "    offset = (0, 0)\n",
    "    sen_spatch = sen[:, centroid[idx][0]-int(20) + offset[0]:centroid[idx][0]+int(20) + offset[0],\n",
    "        centroid[idx][1]-int(20) + offset[1]:centroid[idx][1]+int(20) + offset[1]]\n",
    "    transform = T.CenterCrop(10)\n",
    "    sen_aerialpatch = transform(torch.as_tensor(np.expand_dims(sen_spatch, axis=0))).numpy()\n",
    "    sen = np.transpose(sen, (1,2,0))\n",
    "    sen_spatch = np.transpose(sen_spatch, (1,2,0))\n",
    "    sen_aerialpatch = np.transpose(sen_aerialpatch[0], (1,2,0))\n",
    "\n",
    "    #axs = axs if isinstance(axs[], np.ndarray) else [axs]\n",
    "    ax0 = axs[0][0] ; ax0.imshow(im);ax0.axis('off')\n",
    "    ax1 = axs[0][1] ; ax1.imshow(mk, interpolation='nearest') ;ax1.axis('off')\n",
    "    ax2 = axs[0][2] ; ax2.imshow(im); ax2.imshow(mk, interpolation='nearest', alpha=0.25); ax2.axis('off')\n",
    "    ax3 = axs[1][0] ; ax3.imshow(sen);ax3.axis('off')\n",
    "    ax4 = axs[1][1] ; ax4.imshow(sen_spatch);ax4.axis('off')\n",
    "    ax5 = axs[1][2] ; ax5.imshow(sen_aerialpatch);ax5.axis('off')\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rect = Rectangle((centroid[idx][1]-5.12, centroid[idx][0]-5.12), 10.24, 10.24, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax3.add_patch(rect)\n",
    "    rect = Rectangle((14.88, 14.88), 10.24, 10.24, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax4.add_patch(rect)\n",
    "    \n",
    "    ax0.set_title('RGB Image', size=12,fontweight=\"bold\",c='w')\n",
    "    ax1.set_title('Ground Truth Mask', size=12,fontweight=\"bold\",c='w')\n",
    "    ax2.set_title('Overlay Image & Mask', size=12,fontweight=\"bold\",c='w')\n",
    "    ax3.set_title('Sentinel super area', size=12,fontweight=\"bold\",c='w')\n",
    "    ax4.set_title('Sentinel super patch', size=12,fontweight=\"bold\",c='w')\n",
    "    ax5.set_title('Sentinel over the aerial patch', size=12,fontweight=\"bold\",c='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e5c25-efa8-43d9-96bb-bf7b22510ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample(train_aerial_images, train_labels, train_sentinel_images, train_centroids, idx=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e28eb-3451-427a-b047-5f44ea328957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image 2\n",
    "# Image 5\n",
    "# Image 9 - clear half pixel shift\n",
    "# Image 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016fb7b-7937-4be5-b042-aff0c65b8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-100, 100)\n",
    "\n",
    "#-int(20):int(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22fac1-ad3b-47ba-889c-25ff5861ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
