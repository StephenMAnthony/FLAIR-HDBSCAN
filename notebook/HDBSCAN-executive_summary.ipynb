{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca349dff-0c9f-4cb6-9488-edaa728b4112",
   "metadata": {
    "id": "f631ae2d-c169-461c-937a-3446a78abe79",
    "tags": []
   },
   "source": [
    "# <center><strong>HDSCAN Clustering of FLAIR Data</strong></center>\n",
    "## <center><strong>Complete Analysis</strong></center>\n",
    "<br/>\n",
    "\n",
    "<br/><center>This notebook provides a view of the exploratory project to test whether HDBSCAN clustering can serve as a replacement for manual annotation. </center>\n",
    "<br/> <br/> \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193bbbe-5a71-4941-954b-9790a519506a",
   "metadata": {},
   "source": [
    "<hr style=\"height:1.5px;border-width:0;color:red;background-color:red\">    \n",
    "\n",
    "# <font color='red'>PART-1: Data visualization with the toy dataset training data</font>\n",
    "\n",
    "Note, that this project uses the FLAIR #2 dataset, a publicly available dataset. A reference implementation (including a baseline model) is available in a GitHub repository. The baseline model uses a two-branch architecture integrating a U-Net with a pre-trained ResNet34 encoder and a U-TAE encompassing a temporal self-attention encoder. This project experiments with an alternative data analysis technique, HDBSCAN. As such, this notebook does not use the baseline model. Part 1 uses the reference implementation with minor modifications to load and display the unanalyzed data. The novel portion of this project starts at Part 2. \n",
    "\n",
    "Links\n",
    "Datapaper: https://arxiv.org/pdf/2305.14467.pdf\n",
    "Dataset link: https://ignf.github.io/FLAIR/#FLAIR2\n",
    "Reference Source code link: https://github.com/IGNF/FLAIR-2/tree/main\n",
    "Challenge page: https://codalab.lisn.upsaclay.fr/competitions/13447\n",
    "\n",
    "<p dir=\"auto\">Citation required when using the FLAIR #2 dataset:</p>\n",
    "<div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{ign2023flair2,\n",
    "      title={FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery}, \n",
    "      author={Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos},\n",
    "      year={2023},\n",
    "      booktitle={Advances in Neural Information Processing Systems (NeurIPS) 2023},\n",
    "      doi={https://doi.org/10.48550/arXiv.2310.13336},\n",
    "}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">ign2023flair2</span>,\n",
    "      <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery<span class=\"pl-pds\">}</span></span>, \n",
    "      <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poupée and Sébastien Giordano and Boris Wattrelos<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Advances in Neural Information Processing Systems (NeurIPS) 2023<span class=\"pl-pds\">}</span></span>,\n",
    "      <span class=\"pl-s\">doi</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://doi.org/10.48550/arXiv.2310.13336<span class=\"pl-pds\">}</span></span>,\n",
    "}</pre></div>\n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bd9f5-ba60-4bc2-95bc-895ba173bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcac48-0340-47a6-a224-251ec2083a3e",
   "metadata": {},
   "source": [
    "Import code from or based upon the FLAIR-2 reference implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a23061-65fd-418f-a471-cd22d612ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAIR_path = join(Path.cwd().parents[0],'FLAIR-code/src')\n",
    "if FLAIR_path not in sys.path:\n",
    "    sys.path.append(FLAIR_path)\n",
    "\n",
    "from data_display import (display_nomenclature,\n",
    "                            display_samples, \n",
    "                            display_time_serie,\n",
    "                            display_all_with_semantic_class, \n",
    "                            display_all, \n",
    "                            read_dates, \n",
    "                            filter_dates)\n",
    "from load_data import load_data\n",
    "from FusedDataset import FusedDataset\n",
    "from calc_miou import calc_miou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d1ceb-c611-4878-947a-d8650e9ec1fc",
   "metadata": {
    "id": "88a3b469-b677-4aa2-826e-c00e4476805b",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Nomenclatures</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "The predefined semantic land-cover classes used in the FLAIR #2 datatset. <font color='#90c149'>Two nomenclatures are available </font> : \n",
    "<ul>\n",
    "    <li>the <strong><font color='#90c149'>full nomenclature</font></strong> corresponds to the semantic classes used by experts in photo-interpretation to label the pixels of the ground-truth images.</li>\n",
    "    <li>the <font color='#90c149'><b>main (baseline) nomenclature</b></font> is a simplified version of the full nomenclature. It regroups (into the class 'other') classes that are either strongly under-represented or irrelevant to this challenge.</li>\n",
    "</ul>        \n",
    "See the associated datapaper (https://arxiv.org/pdf/2305.14467.pdf) for additionnal details on these nomenclatures.<br/><br/>\n",
    "\n",
    "<font color='#90c149'>Note:</font> For this project, the reduced nomenclature is used. <br/><hr><br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7c62b-1307-4c73-9db9-8c3054caf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nomenclature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abcbd96-46db-459f-ae0d-51264645c350",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Load Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Use reference code to create lists containing the paths to the input images (`images`) and supervision masks (`masks`) files of the dataset.<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac154f22-ee49-4361-8686-d313f4907db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/app/FLAIR-HDBSCAN/flair-2-config.yml\" \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Creation of the train, val, and test dictionaries with the data file paths\n",
    "# Note that due to using the toy dataset we assign 100% of the data for training. \n",
    "# While not best practice for machine learning, for the toy dataset when using the stock code and less than 100%, issues randomly arise when using the reference loader\n",
    "# as the random selection can result in some semantic classes not being represented. If the full FLAIR #2 dataset were employed, validation data should be separate. \n",
    "# Due to size limitations on HDBSCAN, for actual training we downsample to ~1 % of the training data for training and use 100% of the training data for fitting. \n",
    "d_train, d_val, d_test = load_data(config, val_percent=1)\n",
    "\n",
    "# Create a torch dataset of the training data\n",
    "train_dataset = FusedDataset(dict_files=d_train, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db4752-7a7a-4837-a140-b8b87efc94dc",
   "metadata": {
    "id": "e147039d-d45e-4ab4-a670-7093c603d7ed",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Load an alternate representation of the training data. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe11eb-f9f9-423c-9419-666e43a3b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aerial_images = d_train[\"PATH_IMG\"]\n",
    "train_sentinel_images = d_train[\"PATH_SP_DATA\"]\n",
    "train_labels = d_train[\"PATH_LABELS\"]\n",
    "train_sentinel_masks = d_train[\"PATH_SP_MASKS\"] # Cloud masks\n",
    "train_sentinel_products = d_train[\"PATH_SP_DATES\"] # Needed to get the dates of the sentinel images\n",
    "train_centroids = d_train[\"SP_COORDS\"] # Position of the aerial image in the sentinel super area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bf1cb-ba8a-4efa-a055-1c86c61a2c35",
   "metadata": {
    "id": "48cea476-3bd9-4de8-a562-2b325703d3b7",
    "tags": []
   },
   "source": [
    "## <font color='#90c149'>Visualize Training Data</font>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Display some random samples of image and mask pairs. <font color='#90c149'>Re-run the cell bellow for a different image.</font> Here we also plot the Sentinel super area, super patch and patch. Even though the last one is not used in practice, it is shown to provide an idea of what the Sentinel data looks like. The red rectangle shows the extent of the RGB image inside the Sentinel image. <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53d5c7-a494-4f61-9024-abd851aa7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(train_aerial_images, train_labels, train_sentinel_images, train_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b1407-4bff-4a96-a87b-ef45378fcd37",
   "metadata": {},
   "source": [
    "<br/><hr>\n",
    "We can also plot a few images from sentinel time series along with the acquisition date. Note that some dates may have extensive cloud coverage.<BR>\n",
    "<font color='#90c149'>Re-run the cell below to display sentinel images for different patches.</font>\n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336cf70-3522-443c-b4f8-c5dfb2f26182",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_time_serie(train_sentinel_images, train_sentinel_masks, train_sentinel_products, nb_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76ca4f-6538-4b2c-865e-e90219ef7523",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<br/><hr>\n",
    "\n",
    "Next let's have a closer look at some specific semantic class.<br/> \n",
    "By setting `semantic_class` to a class number (*e.g.*, `semantic_class`=1 for building or `semantic_class`=5 for water) we can visualize the images containing pixels of this specific class. (the full nomenclature is used here.)<br/>\n",
    "<font color='#90c149'>Re-run the cell below with different `semantic_class` values as desired.</font>\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb00841-2be1-4114-bf6e-2738b00606ba",
   "metadata": {
    "id": "9ce3fcb7-8c9b-4207-bdf5-1227ba6600f0"
   },
   "outputs": [],
   "source": [
    "display_all_with_semantic_class(train_aerial_images, train_labels, semantic_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88c4bc-e451-48bb-9817-f02276aa85ce",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "We can directly display all images.<br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8884e-924f-4181-b40f-c519bcab1d7a",
   "metadata": {
    "id": "a3b7d91f-3409-471a-b40c-8ca98be09a3f"
   },
   "outputs": [],
   "source": [
    "display_all(train_aerial_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75bbae-02d7-4a1d-8321-4ac2197b55c7",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-2: Initialize the Exploratory Project </font></center>\n",
    "\n",
    "All code imported from this point onward was developed specifically for this exploratory project. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a61630-2108-4526-ae8b-71de38458183",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = join(Path.cwd().parents[0],'code')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f985b50-b06f-47f2-a8e3-ed50e57c33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import naive\n",
    "reload(naive)\n",
    "from naive import naive_clustering\n",
    "\n",
    "import display\n",
    "reload(display)\n",
    "from display import box_whisker_by_class\n",
    "from display import class_distributions\n",
    "from display import display_normalization_scatter\n",
    "from display import display_confusion\n",
    "from display import plot_timing\n",
    "\n",
    "import classifier\n",
    "reload(classifier)\n",
    "from classifier import extract_spectra\n",
    "from classifier import train_and_validate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fb3d1-7290-4f64-87bd-2b0e49743df9",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-3: Visualizing the Data</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Visualizations of the training data. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37798f7a-c1fb-4dd0-aee2-87a2c7a6cc21",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<hr>\n",
    "Display the class distribution of pixels in the training data. \n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680bacb-129c-4411-b70b-68bc7d0aa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = class_distributions(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d160f8-43b8-4dd7-9f2c-f75f6df96814",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<hr>\n",
    "Display the shape of the dataframe and the first 5 rows. \n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d22122-0022-4861-a6a2-ea1ae556ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = extract_spectra(train_dataset, config, downsample=True, no_other=True, scale_by_intensity=False)\n",
    "print(f\"The dataframe has shape {dataframe.shape}, and the first 5 rows look like below:\")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804d2d6-6db0-447d-a054-b317f8f2c78a",
   "metadata": {
    "id": "f8e4fe44-5f9c-4e69-881b-e24b64c205c8",
    "tags": []
   },
   "source": [
    "<hr>\n",
    "Display the distribution of channel values by semantic classes.<br/> Setting third input (`channel`) to a channel number (*e.g.*, Blue=1, Green=2, Red=3, NIR=4, ..., Elevation=15) displays a box and whisker plot. The box extends from the data's first quartile (Q1) to the third quartile (Q3), where the orange line represents the median. The interquartile distance (IQR) is between Q1 and Q3 (Q3 - Q1). Data points below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are classified as outliers or fliers; such points are displayed individually with circles. Whiskers extend from the box in each direction to the farthest data point which is not an outlier or flier. <BR>\n",
    "<font color='#90c149'>Re-run the cell below with a different value of the third input, `channel`, to see the results for a different channel, band, or elevation.</font>\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796f1c7-bf55-4d34-8162-f297dd118fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_whisker_by_class(dataframe, config, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c1ff3-93c0-4800-b715-307d0db505ab",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "##### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-4: Spectral Normalization</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "Spectral analysis is sometimes improved by separating the spectral profile from the intensity. Normalize all the spectra and add an additional feature corresponding to the intensity. One reason this can be useful is that due to a combination of sun angle and/or off-nadir imaging, there might be shadows. Shadows will generally have a similar spectral shape but different intensity. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ae6d4-78e8-4cdb-8955-d3ad577c7897",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "The two plots below show scatter plot of the distribution of values of the two spectral components without and with spectral normalization. \n",
    "The input channel values (e.g., 'channel1', 'channel2') can be varied using the channel numbers (*e.g.*, Blue=1, Green=2, Red=3, NIR=4, ..., Elevation=15) \n",
    "The raw spectra show a strong correlation between the red and green channels, but comparison with the normalized spectra indicates that much of that correlation is simply a similar dependence upon intensity. When the spectra are normalized, additional grouping and altered distribution is seen. <br/> \n",
    "<font color='#90c149'>Re-run the cell below with a different value(s) of the (`channel1`) and (`channel2`) inputs to see the effect of spectral normalization on different pairs of channels or bands.</font>\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838c7e5-3cad-495d-8244-a7c75a3001e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_normalization_scatter(train_dataset, config, channel1=2, channel2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc79aa-8fd8-4448-8edf-a2946de4c204",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "##### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-5: Models Training and Validation</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "The FLAIR #2 toy dataset is employed, which previously was split into training and test datasets. While best practice is typically to employ training, validation, and test datasets, when working with the toy dataset the random subsetting of the training data into training and validation caused issues as not all classes were always represented in all datasets. Additionally, HDBSCAN (run later) was found to have issues scaling to 1 million pixels. Therefore, rather than randomly assigning some of the 512x512 pixel patches to train and others to validation, we do not assign any patches to validation. Instead, we downsample the training data by a factor of 10 in each dimension before training, effectively using 1% of the training data to train. Validation is performed on the complete training data, of which 99% was not used for training. <br/> \n",
    "\n",
    "Various model configurations can be developed here, with the exact model specified by numerous Boolean parameters. <br>\n",
    "\n",
    "If (`use_hdbscan`) is (`False`), the KNN model will be trained using the manually annotated class labels. If (`use_hdbscan`)<br> is (`True`), autonomously generated labels will be found using HDBSCAN clustering and these labels will be used to train the KNN model. <br>\n",
    "\n",
    "If (`use_satellite`) is (`False`), only the aerial data will be used. If (`use_satellite`)<br> is (`True`), the Sentinel-2 satellite spectra will be appended. <br>\n",
    "\n",
    "If (`scale_by_intensity`) is (`False`), spectral normalization is not employed. If (`scale_by_intensity`)<br> is (`True`), spectral normalization is employed. <br>\n",
    "\n",
    "If (`append_intensity`) is (`False`), nothing happens. If (`append_intensity`)<br> is (`True`) AND (`scale_by_intensity`)<br> is (`True`), the aerial intensity is appended as an additional feature. <br>\n",
    "\n",
    "If (`robust_scale`) is (`False`), RobustScaler() is not employed. If (`robust_scale`)<br> is (`True`), RobustScaler() is employed. <br>\n",
    "\n",
    "If (`check_reliability`) is (`False`), nothing happens. If (`check_reliability`)<br> is (`True`), additional calculations are performed and some reliability metrics are displayed. <br>\n",
    "\n",
    "The recommended (best mIoU scores) models for both when (`use_hdbscan`) is (`False`) and when (`use_hdbscan`) is (`True`) are set to run below by default. \n",
    "\n",
    "<font color='#ff0000'>Warning: The steps in this section can be very computationally expensive. Configuration should allow 8 GB of RAM, and with a normal CPU each cell may take 10-20 minutes to complete.</font>\n",
    "\n",
    "<font color='#90c149'>Re-run the two cells below with a different value(s) of the inputs as specified above to try different combinations of settings as desired.</font>\n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b207c-3638-4ed2-835a-bada90956e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_model = train_and_validate_model(train_dataset, config, use_hdbscan=False, use_satellite=True, scale_by_intensity=True, append_intensity=False, robust_scale=False, check_reliability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e1bee-bc15-42a2-ac57-d77bd6c20e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdbscan_model = train_and_validate_model(train_dataset, config, use_hdbscan=True, use_satellite=True, scale_by_intensity=True, append_intensity=False, robust_scale=False, check_reliability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b795026-9711-49ed-a85f-cb1a8984a31a",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrices and MIOU metrics for the models run above. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44ef30-e963-43c7-b0c9-1a46091dcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(knn_model, config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfb1cc-656a-44d4-bb36-5883fdac5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(hdbscan_model, config);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c9501-214f-4954-9fd3-7e87b2e698ab",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-6: Computational Time</font></center>\n",
    "\n",
    "<br/><hr>\n",
    "This section generates figures of the computational time. The computational time must be manually recorded from runs in Part 5. The prepopulated values correspond to the times obtained running in a Docker container configured to use 8 GB of RAM and 8 cores on a Ryzen 7 1700. \n",
    "<br/> \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552fcad-c5c2-4ab5-ada7-1d3f491b9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {\n",
    "        'KNN alone': (10.0 + 11/60, 10.0 + 18/60, 10.0 + 3/60, 10.0 + 16/60, 10.0 + 30/60, 11.0 + 3/60),\n",
    "        'HDBSCAN + KNN': (11.0 + 9/60, 11.0 + 6/60, 10.0 + 57/60, 11.0 + 4/60, 11.0 + 10/60, 11.0 + 44/60),\n",
    "    }\n",
    "plot_timing(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08f89e-dcc5-490c-8771-a911b2a5b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {\n",
    "        'KNN alone': (12.0 + 42/60, 12.0 + 7/60, 11.0 + 22/60, 13.0 + 55/60, 9*60.0 + 15.0, 9*60.0 + 19),\n",
    "        'HDBSCAN + KNN': (13.0 + 17/60, 13.0 + 9/60, 12.0 + 53/60, 15.0 + 6/60, 4*60.0 + 25.0, 4*60.0 + 57.0),\n",
    "    }\n",
    "plot_timing(times, use_satellite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830cd6d-7868-45c3-9c3a-b3fd1de9d6c2",
   "metadata": {
    "id": "a0bc22ea-1ef0-4f21-aa8c-171807d11362",
    "tags": []
   },
   "source": [
    "###### <br><br>\n",
    "<hr style=\"height:3px;border-width:0;color:red;background-color:red\">   \n",
    "\n",
    "# <center><font color='red'>PART-7: Trivial Algorithms for Comparison </font></center>\n",
    "\n",
    "<br/><hr>\n",
    "\n",
    "In this section, I calculate performance metrics for results obtainable through random chance:<br>\n",
    "1) Randomly assign each pixel to one of the semantic classes, with uniform distribution across the classes. \n",
    "2) Randomly assign each pixel to one of the semantic classes, with the probability of being assigned to a class equal to the prevalence of that class. \n",
    "\n",
    "<hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9435c-d53f-437e-ae02-80e009167034",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = naive_clustering(train_dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00109880-8a2e-42e0-8dc3-649d586503f6",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for trivial implementation #1, uniform distribution. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd013de1-6fb6-4950-b945-03074757a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['random_dict'], config);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43c3a1-f078-4b51-8727-a0c4c0ac3dff",
   "metadata": {
    "id": "da00bce4-c1da-4843-9705-e573a74a7dd9",
    "tags": []
   },
   "source": [
    "<br/><hr> \n",
    "\n",
    "Display the confusion matrix and MIOU metric for trivial implementation #2, distribution with matched prevalence. <br/> <hr><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65330fa6-effe-4a73-b2c4-31264ba0fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion(predictions_dict['permuted_dict'], config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386bc52-f779-45d7-8126-4a27f6ee3df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
